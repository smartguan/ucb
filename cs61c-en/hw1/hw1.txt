Problem 1: Basics Potpourri
1. servers
2. petabytes
3. supercomputers
4. virtual worlds
5. CPU
6. RAM
7. low-end servers
8. milticore processors
9. datacenter
10. embedded computers
11. compiler
12. VHDL
13. desktop computers
14. assembler
15. fortran
16. instruction
17. machine language
18. cobol
19. assembly language
20. operating system
21. application software
22. system software
23. C
24. high-level language
25. terabytes
26. bit



b.  t = (256*2*4)/1024/1024/1 s = 0.002s

c.  t(DRAM) = 2*10^(-2)s,  t(disk) = 2*10^3s,  t(FM) = 2s

d.  1."Fiva nines" talks about the network system availablity in the sense of probability over a year.
    2.There will be 0.001% time of a year in which the system will be down, which is 5.26mins.

e.  Since having failures in a computer warehouse is highly possible and happens every minite and fixing the 
    failing part will take time, warehouse computers has to be designed in a rule such that the outcome result
	always follows the major output, which means the minor fault outputs will be elimited.(e.g. 3 computers in 
	total, 1 fails but 2 other are normal. In this case the final output is still correct by outputing the major
	result)
	
f.  Google uses hardware-grade servers because it's much cheaper than using software-grade servers. However, Google
    itself has to deal with the failures generated by the hardare servers.
	
	







Problem 2: MapReduce

1. Not everything can be done in parallel. For example, If a list contains data of the weight of 10 people 1 month ago and now 
   it needs to be updated, then the parallelizable is not a good fit for Map/Reduce because some people may gain weight while
   some other may lose weight.
  
2. a. Intermediate pairs: (word, intermediate-value) (e.g. ((is, 10) (no, 2) ...))
   b. mapper: FilePointer file = openFile(document);
              List output;
			  char cat;
			  
              while(!endOfFile(file))
			  {
				cat = catchNextWord(file);
			    if(included?(output, cat))
				{
					#cat +=#cat;
				}
				else 
				{
					appendPair(output, cat, 1);
				}				
			  }

      reducer: while(input != null)
	           {
	             tempList = findPairWithSameKeyOfTheFirstPair(inputList);
	             output = addPair(con(caar(tempList), sum(map(cadr, tempList))));
			     deletePairOfKey(input, caar(tempList));
			   }


3.  Assuming that we have a list of (artical-name, outgoing-link) pairs 
    and (current-link, artical-name) pairs, with initial position to be s and destination to be d.
	
     a. mapper: int clicks=0;
                
                tempInput = copyOf(input);
                startPoint = getPairOfClass(s, tempInput);	
                deletePairFromList(tempInput, startPoint);	
                
				//Loop to form path
				path = s;
				currentPosition = s;
				while((cadr(currentPosition) != d) || (clicks >sizeof(input)))
				{		
					//get the first pair with same link to be its class name.
					tempPair = getPairOfClass(cadr(currentPosition), tempInput);	

					deletePairFromList(tempInput, tempPair);
					appendPairToList(path, tempPair);
				    clicks += clicks;
				}
				
				//Construct output pair
				output = cons(path, clicks/2);
	
	            //Put all paths in the increasing order
	 b. Reducer: while(intermediateValue != null)
	             {
	              while(car(intermediateValue) != null)
				  {
					tempPair = getSmaller(car (intermediateValue),
					                     caadr (intermediateValue));
				  }
				 
   				  ouputList = (ouputList, tempPair)
				  intermediateValue = cadr(intermediateValue);
				 }
				
		extra functions: getSmaller(p1, p2)
		                 {
							return (cadr(p1)<cadr(p2)? p1:p2)
						 }